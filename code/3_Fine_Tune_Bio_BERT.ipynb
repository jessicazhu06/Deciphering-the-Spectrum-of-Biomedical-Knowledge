{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning BioBERT Model using DrugProt Dataset"
      ],
      "metadata": {
        "id": "SaXJqrejsrY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PluvK7vV8vkc",
        "outputId": "608a7a90-0f77-462c-8167-3f8623368f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziM29iAneT4h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Data loading functions\n",
        "\n",
        "def load_drugprot_abstracts(file_path):\n",
        "    \"\"\"\n",
        "    Load abstracts from the DrugProt dataset.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Path to the abstracts file.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary with PMIDs as keys and concatenated title + abstract as values.\n",
        "    \"\"\"\n",
        "    abstracts = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            pmid, title, abstract = line.strip().split('\\t')\n",
        "            abstracts[pmid] = title + ' ' + abstract\n",
        "    return abstracts\n",
        "\n",
        "def load_drugprot_entities(file_path):\n",
        "    \"\"\"\n",
        "    Load entity annotations from the DrugProt dataset.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Path to the entities file.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of tuples containing entity information.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 6:\n",
        "                pmid, entity_id, entity_type, start, end, text = parts\n",
        "                entities.append((pmid, entity_id, entity_type, int(start), int(end), text))\n",
        "            else:\n",
        "                print(f\"Unexpected number of fields in line: {line.strip()}\")\n",
        "    return entities\n",
        "\n",
        "def load_drugprot_relations(file_path):\n",
        "    \"\"\"\n",
        "    Load relation annotations from the DrugProt dataset.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Path to the relations file.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of tuples containing relation information.\n",
        "    \"\"\"\n",
        "    relations = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 4:\n",
        "                pmid, rel_type, arg1, arg2 = parts\n",
        "                relations.append((pmid, rel_type, arg1, arg2))\n",
        "            else:\n",
        "                print(f\"Unexpected number of fields in line: {line.strip()}\")\n",
        "    return relations\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "\n",
        "# Named Entity Recognition (NER) data preparation and dataset creation\n",
        "\n",
        "def prepare_ner_data(entities, abstracts):\n",
        "    \"\"\"\n",
        "    Prepare data for NER training by aligning entity annotations with tokenized text.\n",
        "\n",
        "    Args:\n",
        "    entities (list): List of entity annotations.\n",
        "    abstracts (dict): Dictionary of abstracts.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of tuples containing tokens and their corresponding NER labels.\n",
        "    \"\"\"\n",
        "    ner_data = []\n",
        "    for pmid, abstract_text in tqdm(abstracts.items(), desc=\"Preparing NER data\"):\n",
        "        tokens = abstract_text.split()\n",
        "        labels = ['O'] * len(tokens)  # Initialize all tokens as 'Outside' entities\n",
        "        for e_pmid, _, entity_type, start, end, _ in entities:\n",
        "            if e_pmid == pmid:\n",
        "                entity_tokens = abstract_text[start:end].split()\n",
        "                start_token = len(abstract_text[:start].split())\n",
        "                for i, token in enumerate(entity_tokens):\n",
        "                    if start_token + i < len(labels):\n",
        "                        if i == 0:\n",
        "                            labels[start_token + i] = f'B-{entity_type}'  # Beginning of entity\n",
        "                        else:\n",
        "                            labels[start_token + i] = f'I-{entity_type}'  # Inside of entity\n",
        "        ner_data.append((tokens, labels))\n",
        "    return ner_data\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for NER task using DrugProt data.\n",
        "    Initialize the NERDataset.\n",
        "\n",
        "    Args:\n",
        "    data (list): List of tuples, each containing tokens and their corresponding NER labels.\n",
        "    tokenizer: The tokenizer to use for encoding the text (BioBERT tokenizer).\n",
        "    max_len (int): Maximum length of the input sequence.\n",
        "\n",
        "    The label2id dictionary maps NER labels to integer IDs:\n",
        "    - 'O': Outside of a named entity\n",
        "    - 'B-CHEMICAL': Beginning of a chemical entity\n",
        "    - 'I-CHEMICAL': Inside of a chemical entity\n",
        "    - 'B-GENE-Y': Beginning of a gene/protein entity that can be normalized\n",
        "    - 'I-GENE-Y': Inside of a gene/protein entity that can be normalized\n",
        "    - 'B-GENE-N': Beginning of a gene/protein entity that cannot be normalized\n",
        "    - 'I-GENE-N': Inside of a gene/protein entity that cannot be normalized\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label2id = {'O': 0, 'B-CHEMICAL': 1, 'I-CHEMICAL': 2, 'B-GENE-Y': 3, 'I-GENE-Y': 4, 'B-GENE-N': 5, 'I-GENE-N': 6}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens, labels = self.data[idx]\n",
        "        encoding = self.tokenizer(tokens,\n",
        "                                  is_split_into_words=True,\n",
        "                                  max_length=self.max_len,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  return_tensors='pt')\n",
        "\n",
        "        # Convert string labels to IDs\n",
        "        label_ids = [self.label2id[label] for label in labels]\n",
        "        # Pad or truncate label_ids to match max_len\n",
        "        label_ids = label_ids[:self.max_len] + [self.label2id['O']] * (self.max_len - len(label_ids))\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label_ids)\n",
        "        }\n",
        "\n",
        "\n",
        "# Relationship extraction dataset creation\n",
        "\n",
        "class RelationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for Relation Extraction task using DrugProt data.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.relation2id = {rel: i for i, rel in enumerate(set(d[3] for d in data))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, entity1, entity2, relation = self.data[idx]\n",
        "        # Combine text and entities with special separation tokens\n",
        "        combined_text = f\"{text} [SEP] {entity1} [SEP] {entity2}\"\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            combined_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].flatten(),\n",
        "            'attention_mask': encoded['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.relation2id[relation])\n",
        "        }\n",
        "\n",
        "def prepare_relation_data(relations, abstracts, entities):\n",
        "    \"\"\"\n",
        "    Prepare data for Relation Extraction training.\n",
        "\n",
        "    Args:\n",
        "    relations (list): List of relation annotations.\n",
        "    abstracts (dict): Dictionary of abstracts.\n",
        "    entities (list): List of entity annotations.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of tuples containing text, entity1, entity2, and relation type.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    skipped_no_abstract = 0\n",
        "    skipped_no_entity1 = 0\n",
        "    skipped_no_entity2 = 0\n",
        "\n",
        "    # Create a dictionary for faster entity lookup\n",
        "    entity_dict = {}\n",
        "    for e in entities:\n",
        "        pmid, entity_id, entity_type, start, end, text = e\n",
        "        if pmid not in entity_dict:\n",
        "            entity_dict[pmid] = {}\n",
        "        entity_dict[pmid][entity_id] = text\n",
        "\n",
        "    print(f\"Number of PMIDs in entity_dict: {len(entity_dict)}\")\n",
        "    print(f\"Sample entity_dict entry: {list(entity_dict.items())[0]}\")\n",
        "\n",
        "    for pmid, rel_type, arg1, arg2 in relations:\n",
        "        if pmid not in abstracts:\n",
        "            skipped_no_abstract += 1\n",
        "            continue\n",
        "\n",
        "        text = abstracts[pmid]\n",
        "\n",
        "        # Extract entity IDs from arg1 and arg2\n",
        "        entity1_id = arg1.split(':')[1]\n",
        "        entity2_id = arg2.split(':')[1]\n",
        "\n",
        "        if pmid not in entity_dict:\n",
        "            print(f\"PMID {pmid} not found in entity_dict\")\n",
        "        elif entity1_id not in entity_dict[pmid]:\n",
        "            print(f\"Entity1 ID {entity1_id} not found for PMID {pmid}\")\n",
        "            print(f\"Available entity IDs for this PMID: {list(entity_dict[pmid].keys())}\")\n",
        "\n",
        "        if pmid not in entity_dict or entity1_id not in entity_dict[pmid]:\n",
        "            skipped_no_entity1 += 1\n",
        "            continue\n",
        "\n",
        "        if entity2_id not in entity_dict[pmid]:\n",
        "            skipped_no_entity2 += 1\n",
        "            continue\n",
        "\n",
        "        entity1_text = entity_dict[pmid][entity1_id]\n",
        "        entity2_text = entity_dict[pmid][entity2_id]\n",
        "\n",
        "        data.append((text, entity1_text, entity2_text, rel_type))\n",
        "\n",
        "    print(f\"Total relations: {len(relations)}\")\n",
        "    print(f\"Skipped due to missing abstract: {skipped_no_abstract}\")\n",
        "    print(f\"Skipped due to missing entity1: {skipped_no_entity1}\")\n",
        "    print(f\"Skipped due to missing entity2: {skipped_no_entity2}\")\n",
        "    print(f\"Final number of relations: {len(data)}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Model training function\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Train the model and perform validation.\n",
        "\n",
        "    Args:\n",
        "    model: The model to be trained.\n",
        "    train_loader: DataLoader for training data.\n",
        "    val_loader: DataLoader for validation data.\n",
        "    optimizer: Optimizer for updating model parameters.\n",
        "    scheduler: Learning rate scheduler.\n",
        "    num_epochs (int): Number of training epochs.\n",
        "    device: Device to run the model on (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "    model: The trained model.\n",
        "    \"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR_3IZH6L3-u",
        "outputId": "d654ec2b-7a30-48f9-9759-a5491bf541e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Number of abstracts: 3500\n",
            "Number of entities: 89529\n",
            "Number of relations: 17288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing NER data: 100%|██████████| 3500/3500 [00:29<00:00, 120.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of PMIDs in entity_dict: 3500\n",
            "Sample entity_dict entry: ('11808879', {'T1': 'diazoxide', 'T2': 'Diazoxide', 'T3': 'diazoxide', 'T4': 'glutamate', 'T5': 'glucose', 'T6': 'glucose', 'T7': 'diazoxide', 'T8': 'insulin', 'T9': 'SUR1', 'T10': 'KIR6.2', 'T11': 'SUR1', 'T12': 'KIR6.2', 'T13': 'glutamate dehydrogenase', 'T14': 'glucokinase'})\n",
            "Total relations: 17288\n",
            "Skipped due to missing abstract: 0\n",
            "Skipped due to missing entity1: 0\n",
            "Skipped due to missing entity2: 0\n",
            "Final number of relations: 17288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/5: 100%|██████████| 197/197 [02:18<00:00,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Training Loss: 0.5718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 22/22 [00:05<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Validation Loss: 0.5037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 197/197 [02:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Training Loss: 0.5155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 22/22 [00:05<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Validation Loss: 0.4948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 197/197 [02:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Training Loss: 0.5019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 22/22 [00:05<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Validation Loss: 0.4854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 197/197 [02:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Training Loss: 0.4914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 22/22 [00:05<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Validation Loss: 0.4768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 197/197 [02:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Training Loss: 0.4839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 22/22 [00:05<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Validation Loss: 0.4741\n",
            "Number of relation classes: 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/5: 100%|██████████| 973/973 [11:40<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Training Loss: 1.3901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 109/109 [00:25<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Validation Loss: 1.1036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 973/973 [11:39<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Training Loss: 1.0215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 109/109 [00:25<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Validation Loss: 1.0073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 973/973 [11:40<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Training Loss: 0.9024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 109/109 [00:25<00:00,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Validation Loss: 0.9832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 973/973 [11:39<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Training Loss: 0.8305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 109/109 [00:25<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Validation Loss: 0.9604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 973/973 [11:39<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Training Loss: 0.7787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 109/109 [00:25<00:00,  4.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Validation Loss: 0.9650\n",
            "Training completed. Models saved in Google Drive.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    abstracts = load_drugprot_abstracts('/content/drive/MyDrive/drugprot-data/training/drugprot_training_abstracts.tsv')\n",
        "    entities = load_drugprot_entities('/content/drive/MyDrive/drugprot-data/training/drugprot_training_entities.tsv')\n",
        "    relations = load_drugprot_relations('/content/drive/MyDrive/drugprot-data/training/drugprot_training_relations.tsv')\n",
        "\n",
        "    # Print some debug information\n",
        "    print(f\"Number of abstracts: {len(abstracts)}\")\n",
        "    print(f\"Number of entities: {len(entities)}\")\n",
        "    print(f\"Number of relations: {len(relations)}\")\n",
        "\n",
        "    # Prepare data\n",
        "    ner_data = prepare_ner_data(entities, abstracts)\n",
        "    relation_data = prepare_relation_data(relations, abstracts, entities)\n",
        "\n",
        "    if len(relation_data) == 0:\n",
        "        print(\"No valid relations found. Check your data and the prepare_relation_data function.\")\n",
        "    else:\n",
        "        # Split data\n",
        "        ner_train, ner_val = train_test_split(ner_data, test_size=0.1, random_state=42)\n",
        "        relation_train, relation_val = train_test_split(relation_data, test_size=0.1, random_state=42)\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "\n",
        "        # Define model save paths\n",
        "        ner_model_save_path = '/content/drive/MyDrive/trained_ner_model'\n",
        "        relation_model_save_path = '/content/drive/MyDrive/trained_relation_model'\n",
        "        save_path_tokenizer = '/content/drive/MyDrive/tokenizer'\n",
        "\n",
        "        # NER model\n",
        "        ner_model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-v1.1\", num_labels=7).to(device)\n",
        "        ner_train_dataset = NERDataset(ner_train, tokenizer, max_len=256)\n",
        "        ner_val_dataset = NERDataset(ner_val, tokenizer, max_len=256)\n",
        "        ner_train_loader = DataLoader(ner_train_dataset, batch_size=16, shuffle=True)\n",
        "        ner_val_loader = DataLoader(ner_val_dataset, batch_size=16)\n",
        "\n",
        "        ner_optimizer = AdamW(ner_model.parameters(), lr=2e-5)\n",
        "        ner_scheduler = get_linear_schedule_with_warmup(ner_optimizer, num_warmup_steps=0, num_training_steps=len(ner_train_loader) * 5)\n",
        "\n",
        "        trained_ner_model = train_model(ner_model, ner_train_loader, ner_val_loader, ner_optimizer, ner_scheduler, num_epochs=5, device=device)\n",
        "        trained_ner_model.save_pretrained(model_save_path)\n",
        "\n",
        "        # Add this before initializing the relation model\n",
        "        num_relation_classes = len(set(r[3] for r in relation_data))\n",
        "        print(f\"Number of relation classes: {num_relation_classes}\")\n",
        "\n",
        "        # Relation extraction model\n",
        "        relation_model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-v1.1\", num_labels=num_relation_classes).to(device)\n",
        "        relation_train_dataset = RelationDataset(relation_train, tokenizer, max_len=256)\n",
        "        relation_val_dataset = RelationDataset(relation_val, tokenizer, max_len=256)\n",
        "        relation_train_loader = DataLoader(relation_train_dataset, batch_size=16, shuffle=True)\n",
        "        relation_val_loader = DataLoader(relation_val_dataset, batch_size=16)\n",
        "\n",
        "        relation_optimizer = AdamW(relation_model.parameters(), lr=2e-5)\n",
        "        relation_scheduler = get_linear_schedule_with_warmup(relation_optimizer, num_warmup_steps=0, num_training_steps=len(relation_train_loader) * 5)\n",
        "\n",
        "        trained_relation_model = train_model(relation_model, relation_train_loader, relation_val_loader, relation_optimizer, relation_scheduler, num_epochs=5, device=device)\n",
        "        trained_relation_model.save_pretrained(relation_model_save_path)\n",
        "        tokenizer.save_pretrained(save_path_tokenizer)\n",
        "\n",
        "        print(\"Training completed. Models saved in Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxDUlSVSb0IS"
      },
      "outputs": [],
      "source": [
        "trained_ner_model.save_pretrained(ner_model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ThxzPQzd8B7",
        "outputId": "43289ac0-0516-4298-9fda-1b1d3e01aff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw entity data sample:\n",
            "11808879\tT1\tCHEMICAL\t1165\t1174\tdiazoxide\n",
            "11808879\tT2\tCHEMICAL\t1450\t1459\tDiazoxide\n",
            "11808879\tT3\tCHEMICAL\t1901\t1910\tdiazoxide\n",
            "11808879\tT4\tCHEMICAL\t1993\t2002\tglutamate\n",
            "11808879\tT5\tCHEMICAL\t917\t924\tglucose\n",
            "\n",
            "Raw relation data sample:\n",
            "23017395\tINHIBITOR\tArg1:T15\tArg2:T21\n",
            "23017395\tINHIBITOR\tArg1:T16\tArg2:T21\n",
            "12181427\tPART-OF\tArg1:T3\tArg2:T22\n",
            "12181427\tINHIBITOR\tArg1:T6\tArg2:T23\n",
            "12181427\tINHIBITOR\tArg1:T7\tArg2:T23\n"
          ]
        }
      ],
      "source": [
        "print(\"Raw entity data sample:\")\n",
        "with open('/content/drive/MyDrive/drugprot-data/training/drugprot_training_entities.tsv', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "        if i == 4:  # Print first 5 lines\n",
        "            break\n",
        "\n",
        "print(\"\\nRaw relation data sample:\")\n",
        "with open('/content/drive/MyDrive/drugprot-data/training/drugprot_training_relations.tsv', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "        if i == 4:  # Print first 5 lines\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVbSpxv1kevM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

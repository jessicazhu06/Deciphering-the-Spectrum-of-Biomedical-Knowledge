{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTnnLW7phEgC",
        "outputId": "e429180a-9dc0-430f-d562-d94ba0fb52f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5zxZfGALPzn"
      },
      "source": [
        "# Data Loading and Initial Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcIhjRyaKGYm",
        "outputId": "598e9ca2-98f7-4c73-dfb3-5bd28c2f247b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial shape of the dataset: (6918932, 5)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Data Loading\n",
        "def read_csv_in_chunks(file_path, chunk_size=100000):\n",
        "    chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
        "    return pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "# Load the datasets\n",
        "df = pd.read_csv('/content/drive/MyDrive/cleaned_data/sampled_pubmed_data.csv')\n",
        "print(f\"Initial shape of the dataset: {df.shape}\")\n",
        "\n",
        "# Load your fine-tuned models and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/tokenizer\")\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/trained_ner_model\")\n",
        "relation_model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/trained_relation_model\")\n",
        "\n",
        "# Set up device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ner_model.to(device)\n",
        "relation_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj3LL-vuF9r5",
        "outputId": "5ed65a6e-9682-4adc-d4e4-45d85f6dcdcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of 10% subset: (691892, 7)\n",
            "Year\n",
            "2019    112128\n",
            "2020    138753\n",
            "2021    147977\n",
            "2022    144040\n",
            "2023    148994\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Ensure 'Year' column exists and is in the correct format\n",
        "if 'Year' not in df.columns:\n",
        "    df['Year'] = pd.to_datetime(df['Date']).dt.year\n",
        "\n",
        "# Stratified sampling\n",
        "def stratified_sample(df, frac=0.1, random_state=42):\n",
        "    return df.groupby('Year', group_keys=False).apply(lambda x: x.sample(frac=frac, random_state=random_state))\n",
        "\n",
        "df_subset = stratified_sample(df, frac=0.1)\n",
        "df_subset.to_csv('/content/drive/MyDrive/cleaned_data/sample_data.csv')\n",
        "print(f\"Shape of 10% subset: {df_subset.shape}\")\n",
        "\n",
        "# Print distribution of years in the subset\n",
        "print(df_subset['Year'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf-N2rl2LOG0"
      },
      "source": [
        "## Text Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxyWnWE9TPiA",
        "outputId": "3984c3cb-3489-4890-b510-b4ea19bfe8d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_sci_sm\")\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eonEvXWvgbbf"
      },
      "source": [
        "# Data Cleaning and NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXHNoBAto25N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import spacy\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "import os\n",
        "\n",
        "\n",
        "# Load spaCy model for abbreviation expansion\n",
        "nlp = spacy.load(\"en_core_sci_sm\")\n",
        "if \"abbreviation_detector\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"abbreviation_detector\")\n",
        "\n",
        "# Set up device and model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/tokenizer\")\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/trained_ner_model\").to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "ner_model.eval()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Basic cleaning\n",
        "    text = re.sub(r'[\\[\\]\\?\\.]', '', text)  # Remove specific punctuation\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Standardize whitespace\n",
        "\n",
        "    # Abbreviation expansion\n",
        "    doc = nlp(text)\n",
        "    expanded_text = []\n",
        "    for token in doc:\n",
        "        if token._.is_abbreviation:\n",
        "            expanded_text.append(token._.long_form.text)\n",
        "        else:\n",
        "            expanded_text.append(token.text)\n",
        "\n",
        "    return \" \".join(expanded_text)\n",
        "\n",
        "def clean_and_process_text(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    # Clean and preprocess text\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    # Check if text is less than 2 words\n",
        "    if len(text.split()) < 2:\n",
        "        return []\n",
        "\n",
        "    # Tokenize and get NER predictions\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = ner_model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()\n",
        "\n",
        "    # Decode tokens and labels\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    id2label = ner_model.config.id2label\n",
        "\n",
        "    # Reconstruct words and assign labels\n",
        "    words = []\n",
        "    labels = []\n",
        "    current_word = \"\"\n",
        "    current_label = \"O\"\n",
        "\n",
        "    for token, pred in zip(tokens, predictions):\n",
        "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "            if current_word:\n",
        "                words.append(current_word)\n",
        "                labels.append(current_label)\n",
        "            current_word = \"\"\n",
        "            current_label = \"O\"\n",
        "            continue\n",
        "        if token.startswith('##'):\n",
        "            current_word += token[2:]\n",
        "        else:\n",
        "            if current_word:\n",
        "                words.append(current_word)\n",
        "                labels.append(current_label)\n",
        "            current_word = token\n",
        "            current_label = id2label[pred]\n",
        "\n",
        "    if current_word:\n",
        "        words.append(current_word)\n",
        "        labels.append(current_label)\n",
        "\n",
        "    return list(zip(words, labels))\n",
        "\n",
        "def process_chunk(chunk, column_name):\n",
        "    try:\n",
        "        chunk[f'processed_{column_name}'] = chunk[column_name].apply(clean_and_process_text)\n",
        "        # Remove rows where processed text is empty (less than 2 words or None)\n",
        "        chunk = chunk[chunk[f'processed_{column_name}'].apply(len) > 0]\n",
        "        return chunk\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing chunk: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_column(df, column_name, chunk_size, temp_folder):\n",
        "    os.makedirs(temp_folder, exist_ok=True)\n",
        "\n",
        "    # Find the last processed chunk\n",
        "    processed_chunks = [int(f.split('_')[-1].split('.')[0]) for f in os.listdir(temp_folder) if f.startswith('processed_chunk_')]\n",
        "    start_chunk = max(processed_chunks) + chunk_size if processed_chunks else 0\n",
        "\n",
        "    for i in tqdm(range(start_chunk, len(df), chunk_size), desc=f\"Processing {column_name}\"):\n",
        "        chunk = df.iloc[i:i+chunk_size]\n",
        "        chunk_file = f'{temp_folder}/processed_chunk_{i}.csv'\n",
        "\n",
        "        processed_chunk = process_chunk(chunk, column_name)\n",
        "\n",
        "        if processed_chunk is not None and not processed_chunk.empty:\n",
        "            try:\n",
        "                processed_chunk.to_csv(chunk_file, index=False)\n",
        "                print(f\"Processed and saved chunk {i} to {i+chunk_size}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving chunk {i} to {i+chunk_size}: {str(e)}\")\n",
        "\n",
        "def combine_chunks(temp_folder):\n",
        "    processed_chunks = []\n",
        "    for filename in os.listdir(temp_folder):\n",
        "        if filename.startswith('processed_chunk_'):\n",
        "            try:\n",
        "                chunk = pd.read_csv(f'{temp_folder}/{filename}')\n",
        "                processed_chunks.append(chunk)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filename}: {str(e)}\")\n",
        "\n",
        "    if processed_chunks:\n",
        "        final_df = pd.concat(processed_chunks, ignore_index=True)\n",
        "        return final_df\n",
        "    else:\n",
        "        print(f\"No processed chunks found in {temp_folder}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main processing\n",
        "if __name__ == '__main__':\n",
        "    # Load your data\n",
        "    df = pd.read_csv('/content/drive/MyDrive/cleaned_data/sample_data.csv')\n",
        "\n",
        "    temp_folder_titles = '/content/drive/MyDrive/temp_chunks_titles'\n",
        "    temp_folder_abstracts = '/content/drive/MyDrive/temp_chunks_abstracts'\n",
        "\n",
        "    # Process titles (will skip if already completed)\n",
        "    process_column(df, 'Title', 5000, temp_folder_titles)\n",
        "\n",
        "    # Process abstracts (will resume from where it left off)\n",
        "    process_column(df, 'Abstract', 3000, temp_folder_abstracts)\n",
        "\n",
        "    # Combine processed titles\n",
        "    processed_titles = combine_chunks(temp_folder_titles)\n",
        "\n",
        "    # Combine processed abstracts\n",
        "    processed_abstracts = combine_chunks(temp_folder_abstracts)\n",
        "\n",
        "    # Combine titles and abstracts\n",
        "    if processed_titles is not None and processed_abstracts is not None:\n",
        "        combined_df = pd.merge(processed_titles, processed_abstracts, on=df.columns.drop(['Title', 'Abstract']).tolist())\n",
        "\n",
        "        # Save the combined processed data\n",
        "        combined_df.to_csv('/content/drive/MyDrive/cleaned_data/final_processed_data.csv', index=False)\n",
        "        print(\"Processing completed. Combined data saved to /content/drive/MyDrive/cleaned_data/final_processed_data.csv\")\n",
        "    else:\n",
        "        print(\"Error: Could not combine processed titles and abstracts.\")"
      ],
      "metadata": {
        "id": "sPbDZ2kzGNxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJeZf-YJfhED"
      },
      "outputs": [],
      "source": [
        "# Delete the columns that are created during data cleaning and NER\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df_final_processed_data = pd.read_csv('/content/drive/MyDrive/final_processed_data.csv')\n",
        "\n",
        "# Drop the specified columns\n",
        "columns_to_drop = ['Unnamed: 0', 'Unnamed: 0.1', 'Title_y', 'Abstract_y']\n",
        "df_final_processed_data = df_final_processed_data.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Rename columns\n",
        "df_final_processed_data = df_final_processed_data.rename(columns={'Title_x': 'Title', 'Abstract_x': 'Abstract'})\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df_final_processed_data.to_csv('/content/drive/MyDrive/processed_pubmed_data_with_entities.csv', index=False)\n",
        "\n",
        "print(\"Duplicated and index columns have been removed, and 'Title_x' and 'Abstract_x' have been renamed.\")\n",
        "print(\"The cleaned data has been saved to 'processed_pubmed_data_with_entities.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmRi-60afq1Q",
        "outputId": "b653f1f9-a9c5-45f4-8f12-9d4aa450982f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 585249 entries, 0 to 585248\n",
            "Data columns (total 8 columns):\n",
            " #   Column              Non-Null Count   Dtype \n",
            "---  ------              --------------   ----- \n",
            " 0   Date                585249 non-null  object\n",
            " 1   Title               585249 non-null  object\n",
            " 2   Abstract            585249 non-null  object\n",
            " 3   MeshHeading         392552 non-null  object\n",
            " 4   Keywords            434748 non-null  object\n",
            " 5   Year                585249 non-null  int64 \n",
            " 6   processed_Title     585249 non-null  object\n",
            " 7   processed_Abstract  585249 non-null  object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 35.7+ MB\n",
            "Year\n",
            "2019     94028\n",
            "2020    115855\n",
            "2021    124417\n",
            "2022    123783\n",
            "2023    127166\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_final_processed_data = pd.read_csv('/content/drive/MyDrive/processed_pubmed_data_with_entities.csv')\n",
        "df_final_processed_data.info()\n",
        "# Print distribution of years after data cleaning and preprocessing\n",
        "print(df_final_processed_data['Year'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ERINOxyiokO"
      },
      "source": [
        "## Further Data Preprocessing - Match Label with Entity Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyOAXu_hhzea"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Union\n",
        "import ast\n",
        "\n",
        "def enhanced_process_ner_results(ner_results: Union[str, List[Tuple[str, str]]]) -> List[Tuple[str, str]]:\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    current_label = None\n",
        "    label_map = {\n",
        "        'LABEL_1': 'CHEMICAL', 'LABEL_2': 'CHEMICAL',\n",
        "        'LABEL_3': 'GENE-Y', 'LABEL_4': 'GENE-Y',\n",
        "        'LABEL_5': 'GENE-N', 'LABEL_6': 'GENE-N'\n",
        "    }\n",
        "\n",
        "    def get_entity_type(label: str) -> str:\n",
        "        return label_map.get(label, 'UNKNOWN')\n",
        "\n",
        "    def clean_entity_text(text: str) -> str:\n",
        "        text = re.sub(r'\\s*-\\s*', '-', text)\n",
        "        text = re.sub(r'\\s+([,.;:!?)])', r'\\1', text)\n",
        "        text = re.sub(r'(\\()\\s+', r'\\1', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def finalize_entity():\n",
        "        if current_entity:\n",
        "            entity_text = ' '.join(token for token, _ in current_entity)\n",
        "            entity_text = clean_entity_text(entity_text)\n",
        "            entity_label = get_entity_type(current_entity[0][1])\n",
        "            entities.append((entity_text, entity_label))\n",
        "\n",
        "    # Convert string representation of list to actual list if necessary\n",
        "    if isinstance(ner_results, str):\n",
        "        try:\n",
        "            ner_results = ast.literal_eval(ner_results)\n",
        "        except:\n",
        "            print(f\"Error parsing: {ner_results[:100]}...\")  # Print first 100 chars for debugging\n",
        "            return []\n",
        "\n",
        "    # Ensure ner_results is a list of tuples\n",
        "    if not isinstance(ner_results, list) or not all(isinstance(item, tuple) and len(item) == 2 for item in ner_results):\n",
        "        print(f\"Invalid format: {ner_results[:5]}...\")  # Print first 5 items for debugging\n",
        "        return []\n",
        "\n",
        "    for token, label in ner_results:\n",
        "        if label == 'LABEL_0':\n",
        "            finalize_entity()\n",
        "            current_entity = []\n",
        "            current_label = None\n",
        "        elif label in label_map:\n",
        "            if not current_entity or get_entity_type(label) != current_label:\n",
        "                finalize_entity()\n",
        "                current_entity = [(token, label)]\n",
        "                current_label = get_entity_type(label)\n",
        "            else:\n",
        "                current_entity.append((token, label))\n",
        "\n",
        "    finalize_entity()\n",
        "    return entities\n",
        "\n",
        "def process_dataframe_column(df: pd.DataFrame, column_name: str) -> pd.Series:\n",
        "    return df[f'processed_{column_name}'].apply(enhanced_process_ner_results)\n",
        "\n",
        "def process_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for column in ['title', 'abstract']:\n",
        "        if f'processed_{column}' in df.columns:\n",
        "            df[f'{column}_entities'] = process_dataframe_column(df, column)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU46V1Gph_OB",
        "outputId": "a32b86e8-b1bc-45f8-d406-337e4fa13359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Title...\n",
            "Error parsing: [('Epigallocatechin', 'LABEL_1'), ('gallate', 'LABEL_1'), ('diminishes', 'LABEL_0'), ('cigarette', '...\n",
            "Finished processing Title.\n",
            "Processing Abstract...\n",
            "Error parsing: [('Entamoeba', 'LABEL_0'), ('histolytica', 'LABEL_0'), ('intestinal', 'LABEL_0'), ('parasite', 'LABE...\n",
            "Error parsing: [('Kidney', 'LABEL_0'), ('care', 'LABEL_0'), ('United', 'LABEL_0'), ('States', 'LABEL_0'), ('highly'...\n",
            "Finished processing Abstract.\n"
          ]
        }
      ],
      "source": [
        "df_final_processed_data = process_dataset(df_final_processed_data)\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df_final_processed_data.to_csv('/content/drive/MyDrive/cleaned_data/cleaned_final_processed_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_final_processed_data = pd.read_csv('/content/drive/MyDrive/cleaned_data/cleaned_final_processed_data.csv')"
      ],
      "metadata": {
        "id": "DALXRVOJ4fG0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_processed_data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "qnAruhfXGlHP",
        "outputId": "9fb0f72b-24a7-4715-9548-49058119bd23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   article_id        Date                                              Title  \\\n",
              "0           0  2019-03-01  Cellular Proteostasis During Influenza A Virus...   \n",
              "1           1  2019-07-01  Next-generation sequencing with comprehensive ...   \n",
              "2           2  2019-12-01  [THE HEALTH SAVING TECHNOLOGIES AT A PEDAGOGIC...   \n",
              "3           3  2019-11-01  An overview of colistin resistance, mobilized ...   \n",
              "4           4  2019-10-01  Decoupling Filamentous Phage Uptake and Energy...   \n",
              "\n",
              "                                            Abstract  \\\n",
              "0  In order to efficiently replicate, viruses req...   \n",
              "1  Familial adenomatous polyposis (FAP) is an aut...   \n",
              "2  The article investigates health saving technol...   \n",
              "3  Colistin, also known as polymyxin E, is an ant...   \n",
              "4  Filamentous phages are nonlytic viruses that s...   \n",
              "\n",
              "                                         MeshHeading  \\\n",
              "0  Humans; Influenza A virus; Influenza, Human; M...   \n",
              "1  Adenomatous Polyposis Coli; Adenomatous Polypo...   \n",
              "2  Biomedical Technology; Educational Technology;...   \n",
              "3                                                NaN   \n",
              "4  Bacterial Proteins; Bacteriophages; Escherichi...   \n",
              "\n",
              "                                            Keywords  Year  \\\n",
              "0  influenza A virus (IAV); protein aggregation; ...  2019   \n",
              "1  APC; Colorectal cancer; Familial adenomatous p...  2019   \n",
              "2                                                NaN  2019   \n",
              "3  Enterobacteriaceae; animals; colistin alternat...  2019   \n",
              "4  Tol-Pal system; Ton system; bacteriophage; bac...  2019   \n",
              "\n",
              "                                     processed_Title  \\\n",
              "0  [('Cellular', 'LABEL_0'), ('Proteostasis', 'LA...   \n",
              "1  [('Next', 'LABEL_0'), ('-', 'LABEL_0'), ('gene...   \n",
              "2  [('THE', 'LABEL_0'), ('HEALTH', 'LABEL_0'), ('...   \n",
              "3  [('An', 'LABEL_0'), ('overview', 'LABEL_0'), (...   \n",
              "4  [('Decoupling', 'LABEL_0'), ('Filamentous', 'L...   \n",
              "\n",
              "                                  processed_Abstract  \\\n",
              "0  [('In', 'LABEL_0'), ('order', 'LABEL_0'), ('to...   \n",
              "1  [('Familial', 'LABEL_0'), ('adenomatous', 'LAB...   \n",
              "2  [('The', 'LABEL_0'), ('article', 'LABEL_0'), (...   \n",
              "3  [('Colistin', 'LABEL_1'), (',', 'LABEL_0'), ('...   \n",
              "4  [('Filamentous', 'LABEL_0'), ('phages', 'LABEL...   \n",
              "\n",
              "                                      title_entities  \\\n",
              "0                                                 []   \n",
              "1                                [('APC', 'GENE-Y')]   \n",
              "2                                                 []   \n",
              "3  [('colistin', 'CHEMICAL'), ('colistin', 'CHEMI...   \n",
              "4                             [('TolQRA', 'GENE-N')]   \n",
              "\n",
              "                                   abstract_entities  \n",
              "0                                                 []  \n",
              "1             [('APC', 'GENE-Y'), ('APC', 'GENE-Y')]  \n",
              "2                                                 []  \n",
              "3  [('Colistin', 'CHEMICAL'), ('polymyxin E', 'CH...  \n",
              "4  [('Tol', 'GENE-N'), ('Tol system', 'GENE-N'), ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da69618f-4398-4d08-9710-170b353635fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>MeshHeading</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Year</th>\n",
              "      <th>processed_Title</th>\n",
              "      <th>processed_Abstract</th>\n",
              "      <th>title_entities</th>\n",
              "      <th>abstract_entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-03-01</td>\n",
              "      <td>Cellular Proteostasis During Influenza A Virus...</td>\n",
              "      <td>In order to efficiently replicate, viruses req...</td>\n",
              "      <td>Humans; Influenza A virus; Influenza, Human; M...</td>\n",
              "      <td>influenza A virus (IAV); protein aggregation; ...</td>\n",
              "      <td>2019</td>\n",
              "      <td>[('Cellular', 'LABEL_0'), ('Proteostasis', 'LA...</td>\n",
              "      <td>[('In', 'LABEL_0'), ('order', 'LABEL_0'), ('to...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2019-07-01</td>\n",
              "      <td>Next-generation sequencing with comprehensive ...</td>\n",
              "      <td>Familial adenomatous polyposis (FAP) is an aut...</td>\n",
              "      <td>Adenomatous Polyposis Coli; Adenomatous Polypo...</td>\n",
              "      <td>APC; Colorectal cancer; Familial adenomatous p...</td>\n",
              "      <td>2019</td>\n",
              "      <td>[('Next', 'LABEL_0'), ('-', 'LABEL_0'), ('gene...</td>\n",
              "      <td>[('Familial', 'LABEL_0'), ('adenomatous', 'LAB...</td>\n",
              "      <td>[('APC', 'GENE-Y')]</td>\n",
              "      <td>[('APC', 'GENE-Y'), ('APC', 'GENE-Y')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2019-12-01</td>\n",
              "      <td>[THE HEALTH SAVING TECHNOLOGIES AT A PEDAGOGIC...</td>\n",
              "      <td>The article investigates health saving technol...</td>\n",
              "      <td>Biomedical Technology; Educational Technology;...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019</td>\n",
              "      <td>[('THE', 'LABEL_0'), ('HEALTH', 'LABEL_0'), ('...</td>\n",
              "      <td>[('The', 'LABEL_0'), ('article', 'LABEL_0'), (...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2019-11-01</td>\n",
              "      <td>An overview of colistin resistance, mobilized ...</td>\n",
              "      <td>Colistin, also known as polymyxin E, is an ant...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Enterobacteriaceae; animals; colistin alternat...</td>\n",
              "      <td>2019</td>\n",
              "      <td>[('An', 'LABEL_0'), ('overview', 'LABEL_0'), (...</td>\n",
              "      <td>[('Colistin', 'LABEL_1'), (',', 'LABEL_0'), ('...</td>\n",
              "      <td>[('colistin', 'CHEMICAL'), ('colistin', 'CHEMI...</td>\n",
              "      <td>[('Colistin', 'CHEMICAL'), ('polymyxin E', 'CH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2019-10-01</td>\n",
              "      <td>Decoupling Filamentous Phage Uptake and Energy...</td>\n",
              "      <td>Filamentous phages are nonlytic viruses that s...</td>\n",
              "      <td>Bacterial Proteins; Bacteriophages; Escherichi...</td>\n",
              "      <td>Tol-Pal system; Ton system; bacteriophage; bac...</td>\n",
              "      <td>2019</td>\n",
              "      <td>[('Decoupling', 'LABEL_0'), ('Filamentous', 'L...</td>\n",
              "      <td>[('Filamentous', 'LABEL_0'), ('phages', 'LABEL...</td>\n",
              "      <td>[('TolQRA', 'GENE-N')]</td>\n",
              "      <td>[('Tol', 'GENE-N'), ('Tol system', 'GENE-N'), ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da69618f-4398-4d08-9710-170b353635fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da69618f-4398-4d08-9710-170b353635fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da69618f-4398-4d08-9710-170b353635fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4156865b-12fe-4c07-9d31-f094f28ecc25\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4156865b-12fe-4c07-9d31-f094f28ecc25')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4156865b-12fe-4c07-9d31-f094f28ecc25 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final_processed_data"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANVUhw-Bgv_V"
      },
      "source": [
        "# Relationship Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyrJ8EF5fq8w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_final_processed_data = pd.read_csv('/content/drive/MyDrive/cleaned_data/processed_pubmed_data_with_entities.csv')\n",
        "\n",
        "def index_dataset(df):\n",
        "    # Create a unique identifier for each row\n",
        "    df['article_id'] = np.arange(len(df))\n",
        "\n",
        "    # Set this identifier as the index\n",
        "    df.set_index('article_id', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply indexing to your dataset\n",
        "df_final_processed_data = index_dataset(df_final_processed_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm, tqdm as tqdm_notebook\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import json\n",
        "import ast\n",
        "\n",
        "# Define relationship types\n",
        "RELATIONSHIP_TYPES = [\n",
        "    'INHIBITOR', 'DIRECT-REGULATOR', 'SUBSTRATE', 'ACTIVATOR', 'INDIRECT-UPREGULATOR',\n",
        "    'INDIRECT-DOWNREGULATOR', 'ANTAGONIST', 'PRODUCT-OF', 'PART-OF', 'AGONIST',\n",
        "    'AGONIST-ACTIVATOR', 'SUBSTRATE_PRODUCT-OF', 'AGONIST-INHIBITOR'\n",
        "]\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer_path = '/content/drive/MyDrive/tokenizer'\n",
        "model_path = '/content/drive/MyDrive/trained_relation_model'\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(RELATIONSHIP_TYPES))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "class DrugTargetDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item['text']\n",
        "        entity1 = item['entity1']\n",
        "        entity2 = item['entity2']\n",
        "\n",
        "        combined_text = f\"{text} [SEP] {entity1} [SEP] {entity2}\"\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            combined_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'entity1': entity1,\n",
        "            'entity2': entity2,\n",
        "            'entity1_type': item['entity1_type'],\n",
        "            'entity2_type': item['entity2_type'],\n",
        "            'article_id': item['article_id']\n",
        "        }\n",
        "\n",
        "# Data cleaning and preprocessing\n",
        "def clean_relationships(df):\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "    # Ensure article_id is string\n",
        "    df['article_id'] = df['article_id'].astype(str)\n",
        "    print(f\"After ensuring article_id is string shape: {df.shape}\")\n",
        "\n",
        "    # Standardize entity names\n",
        "    df['entity1'] = df.apply(lambda row: standardize_entity(row['entity1'], row['entity1_type']), axis=1)\n",
        "    df['entity2'] = df.apply(lambda row: standardize_entity(row['entity2'], row['entity2_type']), axis=1)\n",
        "    print(f\"After standardizing entities shape: {df.shape}\")\n",
        "\n",
        "    # Remove rows with blank entities\n",
        "    df = df.dropna(subset=['entity1', 'entity2'])\n",
        "    print(f\"After removing blank entities shape: {df.shape}\")\n",
        "\n",
        "    # Create a directional pair (maintain original order)\n",
        "    df['entity_pair'] = df.apply(lambda row: (row['entity1'], row['entity2']), axis=1)\n",
        "\n",
        "    # Group by article_id and sort within each group by confidence\n",
        "    df_sorted = df.sort_values(['article_id', 'confidence'], ascending=[True, False])\n",
        "\n",
        "    # Drop duplicates within each article_id group, preserving directionality\n",
        "    df_cleaned = df_sorted.drop_duplicates(subset=['article_id', 'entity_pair', 'relationship'], keep='first')\n",
        "    print(f\"After removing duplicates shape: {df_cleaned.shape}\")\n",
        "\n",
        "    # Drop temporary columns\n",
        "    df_cleaned = df_cleaned.drop(columns=['entity_pair'])\n",
        "\n",
        "    return df_cleaned\n",
        "\n",
        "def standardize_entity(entity, entity_type):\n",
        "    if pd.isna(entity) or entity.strip() == '':\n",
        "        return None\n",
        "\n",
        "    entity = str(entity).strip()\n",
        "\n",
        "    if entity_type == 'CHEMICAL':\n",
        "        return entity.lower()  # Changed to lowercase for CHEMICAL entities\n",
        "    else:\n",
        "        return entity  # Other entity types remain unchanged\n",
        "\n",
        "def safe_eval(entity_string):\n",
        "    try:\n",
        "        return ast.literal_eval(entity_string)\n",
        "    except (ValueError, SyntaxError):\n",
        "        print(f\"Error parsing entity string: {entity_string}\")\n",
        "        return []\n",
        "\n",
        "def process_chunk(chunk, temp_folder, chunk_id):\n",
        "    data = []\n",
        "    for _, row in chunk.iterrows():\n",
        "        article_id = row.name  # This gets the index, which is your article_id\n",
        "        text = f\"{row['Title']} {row['Abstract']}\"\n",
        "        entities = safe_eval(row['title_entities']) + safe_eval(row['abstract_entities'])\n",
        "\n",
        "        for i, e1 in enumerate(entities):\n",
        "            for e2 in entities[i+1:]:\n",
        "                if e1[1] in ['CHEMICAL', 'GENE-Y', 'GENE-N'] and e2[1] in ['CHEMICAL', 'GENE-Y', 'GENE-N']:\n",
        "                    data.append({\n",
        "                        'article_id': str(article_id),  # Convert to string to ensure consistency\n",
        "                        'text': text,\n",
        "                        'entity1': e1[0],\n",
        "                        'entity1_type': e1[1],\n",
        "                        'entity2': e2[0],\n",
        "                        'entity2_type': e2[1]\n",
        "                    })\n",
        "\n",
        "    print(f\"Chunk {chunk_id}: Found {len(data)} valid entity pairs to process\")\n",
        "\n",
        "    dataset = DrugTargetDataset(data, tokenizer, max_length=256)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    predictions = extract_relationships(model, dataloader, device)\n",
        "\n",
        "    cleaned_predictions = clean_relationships(predictions)\n",
        "\n",
        "    chunk_file = f'{temp_folder}/processed_chunk_{chunk_id}.csv'\n",
        "    cleaned_predictions.to_csv(chunk_file, index=False)\n",
        "    print(f\"Processed and saved chunk {chunk_id} with {len(cleaned_predictions)} relationships\")\n",
        "    return len(cleaned_predictions)\n",
        "\n",
        "def extract_relationships(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Extracting relationships\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_relationships = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            for i, pred in enumerate(predicted_relationships):\n",
        "                relationship = RELATIONSHIP_TYPES[pred.item()]\n",
        "                all_predictions.append({\n",
        "                    'article_id': batch['article_id'][i],\n",
        "                    'entity1': batch['entity1'][i],\n",
        "                    'entity1_type': batch['entity1_type'][i],\n",
        "                    'entity2': batch['entity2'][i],\n",
        "                    'entity2_type': batch['entity2_type'][i],\n",
        "                    'relationship': relationship,\n",
        "                    'confidence': probabilities[i][pred].item()\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(all_predictions)\n",
        "\n",
        "# Modify the combine_chunks function to handle potential errors\n",
        "def combine_chunks(temp_folder):\n",
        "    all_predictions = []\n",
        "    for filename in os.listdir(temp_folder):\n",
        "        if filename.startswith('processed_chunk_'):\n",
        "            try:\n",
        "                chunk_df = pd.read_csv(f'{temp_folder}/{filename}')\n",
        "                all_predictions.append(chunk_df)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {filename}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    if not all_predictions:\n",
        "        raise ValueError(\"No valid chunks found. Please check your data and processing steps.\")\n",
        "\n",
        "    combined_df = pd.concat(all_predictions, ignore_index=True)\n",
        "    combined_df.to_csv('/content/drive/MyDrive/cleaned_data/concat_data.csv')\n",
        "    final_cleaned_df = clean_relationships(combined_df)\n",
        "\n",
        "    print(f\"Combined and cleaned {len(all_predictions)} chunks. Total relationships: {len(final_cleaned_df)}\")\n",
        "    return final_cleaned_df\n",
        "\n",
        "def process_full_dataset(df, chunk_size, temp_folder):\n",
        "    os.makedirs(temp_folder, exist_ok=True)\n",
        "    progress_file = os.path.join(temp_folder, 'progress.json')\n",
        "\n",
        "    # Find the highest numbered chunk file\n",
        "    existing_chunks = [int(f.split('_')[-1].split('.')[0]) for f in os.listdir(temp_folder) if f.startswith('processed_chunk_')]\n",
        "\n",
        "    if existing_chunks:\n",
        "        start_chunk = max(existing_chunks) + 1\n",
        "        print(f\"Found {len(existing_chunks)} existing processed chunks.\")\n",
        "    else:\n",
        "        start_chunk = 0\n",
        "\n",
        "    total_relationships = 0\n",
        "    if os.path.exists(progress_file):\n",
        "        try:\n",
        "            with open(progress_file, 'r') as f:\n",
        "                file_content = f.read().strip()\n",
        "                if file_content:\n",
        "                    progress = json.loads(file_content)\n",
        "                    total_relationships = progress.get('total_relationships', 0)\n",
        "                else:\n",
        "                    print(\"Progress file is empty. Starting with 0 total relationships.\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error reading progress file. Starting with 0 total relationships.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error reading progress file: {str(e)}. Starting with 0 total relationships.\")\n",
        "\n",
        "    print(f\"Starting processing from chunk {start_chunk}\")\n",
        "    print(f\"Total relationships so far: {total_relationships}\")\n",
        "\n",
        "    total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
        "    remaining_chunks = max(0, total_chunks - start_chunk)\n",
        "\n",
        "    if remaining_chunks > 0:\n",
        "        pbar = tqdm_notebook(total=remaining_chunks, desc=\"Processing chunks\")\n",
        "        for chunk_id in range(start_chunk, total_chunks):\n",
        "            try:\n",
        "                start_row = chunk_id * chunk_size\n",
        "                end_row = min((chunk_id + 1) * chunk_size, len(df))\n",
        "                chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "                chunk_relationships = process_chunk(chunk, temp_folder, chunk_id)\n",
        "                total_relationships += chunk_relationships\n",
        "\n",
        "                # Update progress\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix({\"Total Relationships\": total_relationships})\n",
        "\n",
        "                # Save progress after each chunk\n",
        "                progress = {\n",
        "                    'last_processed_chunk': chunk_id,\n",
        "                    'total_relationships': total_relationships\n",
        "                }\n",
        "                with open(progress_file, 'w') as f:\n",
        "                    json.dump(progress, f)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nAn error occurred while processing chunk {chunk_id}: {str(e)}\")\n",
        "                print(\"Continuing with the next chunk...\")\n",
        "\n",
        "        pbar.close()\n",
        "    else:\n",
        "        print(\"All chunks have been processed. Moving to the next step.\")\n",
        "\n",
        "    print(\"\\nFinished processing all chunks.\")\n",
        "    return total_relationships\n",
        "\n",
        "# Main execution\n",
        "chunk_size = 100  # Adjust based on your memory constraints\n",
        "temp_folder = '/content/drive/MyDrive/relationship_extraction_temp'\n",
        "\n",
        "print(f\"Starting relationship extraction on dataset with {len(df_final_processed_data)} rows\")\n",
        "total_relationships = process_full_dataset(df_final_processed_data, chunk_size, temp_folder)\n",
        "\n",
        "print(\"\\nCombining all processed chunks...\")\n",
        "final_results = combine_chunks(temp_folder)\n",
        "\n",
        "# Save the final results\n",
        "output_path = '/content/drive/MyDrive/cleaned_data/final_relationship_results.csv'\n",
        "final_results.to_csv(output_path, index=False)\n",
        "print(f\"Relationship extraction completed. Results saved to {output_path}\")\n",
        "print(f\"Total relationships extracted: {len(final_results)}\")\n",
        "\n",
        "# Analysis of cleaning impact\n",
        "print(\"\\nAnalysis of cleaning impact:\")\n",
        "print(f\"Number of unique articles: {final_results['article_id'].nunique()}\")\n",
        "print(\"\\nTop 10 most frequent relationships:\")\n",
        "print(final_results['relationship'].value_counts().head(10))"
      ],
      "metadata": {
        "id": "YIUOVBr-GIfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Data Cleaning on Relationship Extraction Results"
      ],
      "metadata": {
        "id": "w86t_t_RK_Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Define a set of common stopwords\n",
        "stopwords = set(['and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
        "\n",
        "def is_valid_entity(entity):\n",
        "    # Remove leading/trailing whitespace\n",
        "    entity = entity.strip().lower()\n",
        "\n",
        "    # Check if entity is empty, consists only of punctuation, or is a stopword\n",
        "    if not entity or re.match(r'^[-(),/.>{}\\']+$', entity) or entity in stopwords:\n",
        "        print(f\"Removed invalid entity: '{entity}'\")\n",
        "        return False\n",
        "\n",
        "    # Check if entity consists only of numbers\n",
        "    if re.match(r'^\\d+$', entity):\n",
        "        print(f\"Removed number-only entity: '{entity}'\")\n",
        "        return False\n",
        "\n",
        "    # Punctuations that shouldn't appear at the start\n",
        "    invalid_start = r'^[-\\]\\)-,/.>\\'%&]'\n",
        "\n",
        "    # Punctuations that shouldn't appear at the end\n",
        "    invalid_end = r'[-,(/.>\\'{$%]$'\n",
        "\n",
        "    # Check if entity starts with invalid punctuation\n",
        "    if re.match(invalid_start, entity):\n",
        "        print(f\"Removed entity starting with invalid punctuation: '{entity}'\")\n",
        "        return False\n",
        "\n",
        "    # Check if entity ends with invalid punctuation\n",
        "    if re.search(invalid_end, entity):\n",
        "        print(f\"Removed entity ending with invalid punctuation: '{entity}'\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Load the data\n",
        "final_results = pd.read_csv('/content/drive/MyDrive/cleaned_data/final_relationship_results.csv')\n",
        "\n",
        "# Filter rows based on entity validity\n",
        "cleaned_final_results = final_results[final_results['entity1'].apply(is_valid_entity) & final_results['entity2'].apply(is_valid_entity)]\n",
        "\n",
        "# Save the filtered data\n",
        "cleaned_final_results.to_csv('/content/drive/MyDrive/cleaned_data/pubmed_data_with_relationships.csv', index=False)\n",
        "\n",
        "# Print some examples of removed entities\n",
        "print(\"\\nExamples of removed entities:\")\n",
        "for idx, row in final_results.iterrows():\n",
        "    if not is_valid_entity(row['entity1']):\n",
        "        print(f\"Removed entity1: '{row['entity1']}'\")\n",
        "    if not is_valid_entity(row['entity2']):\n",
        "        print(f\"Removed entity2: '{row['entity2']}'\")\n",
        "    if idx > 20:  # Limit to first 20 rows for brevity\n",
        "        break\n",
        "\n",
        "print(f\"\\nData filtering completed. {len(final_results) - len(cleaned_final_results)} rows removed.\")\n",
        "print(f\"Filtered data saved to 'pubmed_data_with_relationships.csv' with {len(cleaned_final_results)} rows.\")\n",
        "\n",
        "# Check for any remaining entities starting with punctuation\n",
        "remaining_invalid_start = cleaned_final_results[cleaned_final_results['entity1'].str.match(r'^[-\\]\\)-,/.>\\'%&]') |\n",
        "                                                cleaned_final_results['entity2'].str.match(r'^[-\\]\\)-,/.>\\'%&]')]\n",
        "if not remaining_invalid_start.empty:\n",
        "    print(\"\\nWarning: Some entities still start with invalid punctuation:\")\n",
        "    print(remaining_invalid_start[['entity1', 'entity2']].head())"
      ],
      "metadata": {
        "id": "S_H5XqcgF4jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_final_results.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hRXqAbs4G4vf",
        "outputId": "a7ca546a-fd8b-4ad1-b32e-67d3a0e4a4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   article_id                      entity1 entity1_type  \\\n",
              "0           1                          APC       GENE-Y   \n",
              "1          10                  ion channel       GENE-N   \n",
              "2          10                  ion channel       GENE-N   \n",
              "3          10                  ion channel       GENE-N   \n",
              "4          10  G protein-coupled receptors       GENE-N   \n",
              "\n",
              "                       entity2 entity2_type relationship  confidence  \n",
              "0                          APC       GENE-Y      AGONIST    0.592381  \n",
              "1  G protein-coupled receptors       GENE-N      AGONIST    0.567602  \n",
              "2                         GPCR       GENE-N      AGONIST    0.567602  \n",
              "3             nuclear receptor       GENE-N      AGONIST    0.567602  \n",
              "4                         GPCR       GENE-N      AGONIST    0.567602  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c65a209b-df56-43f2-9ef1-249c70753ba9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>entity1</th>\n",
              "      <th>entity1_type</th>\n",
              "      <th>entity2</th>\n",
              "      <th>entity2_type</th>\n",
              "      <th>relationship</th>\n",
              "      <th>confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>APC</td>\n",
              "      <td>GENE-Y</td>\n",
              "      <td>APC</td>\n",
              "      <td>GENE-Y</td>\n",
              "      <td>AGONIST</td>\n",
              "      <td>0.592381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>ion channel</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>G protein-coupled receptors</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>AGONIST</td>\n",
              "      <td>0.567602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>ion channel</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>GPCR</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>AGONIST</td>\n",
              "      <td>0.567602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>ion channel</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>nuclear receptor</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>AGONIST</td>\n",
              "      <td>0.567602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>G protein-coupled receptors</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>GPCR</td>\n",
              "      <td>GENE-N</td>\n",
              "      <td>AGONIST</td>\n",
              "      <td>0.567602</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c65a209b-df56-43f2-9ef1-249c70753ba9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c65a209b-df56-43f2-9ef1-249c70753ba9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c65a209b-df56-43f2-9ef1-249c70753ba9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-021ffa61-8100-4ad6-87e5-c0278f6867ea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-021ffa61-8100-4ad6-87e5-c0278f6867ea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-021ffa61-8100-4ad6-87e5-c0278f6867ea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "relationship_df"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Next Step: Novel Targets Identification and Polypharmacology Analysis*"
      ],
      "metadata": {
        "id": "jnj-2RNyZVng"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

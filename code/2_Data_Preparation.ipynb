{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data from the Whole Dataset"
      ],
      "metadata": {
        "id": "f9c7nkpVyeWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample data from 2019 to 2023 using chunk"
      ],
      "metadata": {
        "id": "xlzQzOvZ5ukG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghJgKFSCx-w-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the directory of the script\n",
        "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    try:\n",
        "        # Convert 'Date' to datetime\n",
        "        chunk['Date'] = pd.to_datetime(chunk['Date'], format='%Y-%b', errors='coerce')\n",
        "\n",
        "        # Fill missing dates with the last valid date\n",
        "        chunk['Date'] = chunk['Date'].fillna(method='ffill')\n",
        "\n",
        "        # Filter for dates between 2019 and 2023\n",
        "        mask = (chunk['Date'] >= '2019-01-01') & (chunk['Date'] <= '2023-12-31')\n",
        "        return chunk.loc[mask]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing chunk: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def process_file(input_file, output_file):\n",
        "    chunksize = 100000  # Increased chunk size\n",
        "\n",
        "    with open(output_file, 'w') as temp_file:\n",
        "        header_written = False\n",
        "        for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunksize, engine='python', on_bad_lines='skip')):\n",
        "            try:\n",
        "                processed_chunk = process_chunk(chunk)\n",
        "                if not processed_chunk.empty:\n",
        "                    processed_chunk.to_csv(temp_file, header=not header_written, index=False)\n",
        "                    header_written = True  # Ensure header is written only once\n",
        "                print(f\"Processed chunk {i} from {input_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing chunk {i} from {input_file}: {e}\")\n",
        "\n",
        "    print(f\"Processed data saved to {output_file}\")\n",
        "\n",
        "# File names\n",
        "large_file = 'large_file.csv'\n",
        "small_file = 'small_file.csv'\n",
        "\n",
        "# Full paths\n",
        "large_file_path = os.path.join(script_dir, large_file)\n",
        "small_file_path = os.path.join(script_dir, small_file)\n",
        "\n",
        "# Check if files exist\n",
        "if not os.path.exists(large_file_path):\n",
        "    print(f\"Error: {large_file} not found in the script directory.\")\n",
        "    sys.exit(1)\n",
        "if not os.path.exists(small_file_path):\n",
        "    print(f\"Error: {small_file} not found in the script directory.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Process each file\n",
        "for i, file in enumerate([large_file_path, small_file_path]):\n",
        "    output_file = os.path.join(script_dir, f'processed_file_{i+1}.csv')\n",
        "    process_file(file, output_file)\n",
        "\n",
        "print(\"Processing complete. Please check the output CSV files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the datasets are quite large, we process the full dataset using chunk and save in several smaller files. Now we need to combine them and get the final sampled data."
      ],
      "metadata": {
        "id": "X9wqPhApzPfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    # Combine title and abstract\n",
        "    if 'title' in chunk.columns and 'abstract' in chunk.columns:\n",
        "        chunk['full_text'] = chunk['title'] + ' ' + chunk['abstract'].fillna('')\n",
        "        chunk['full_text'] = chunk['full_text'].str.lower()\n",
        "\n",
        "    # Process MeSH terms and keywords if they exist\n",
        "    if 'meshheading' in chunk.columns:\n",
        "        chunk['mesh_terms'] = chunk['meshheading'].str.split(';')\n",
        "    if 'keywords' in chunk.columns:\n",
        "        chunk['keywords'] = chunk['keywords'].str.split(';')\n",
        "\n",
        "    # Convert date to datetime if it exists\n",
        "    if 'date' in chunk.columns:\n",
        "        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n",
        "\n",
        "    return chunk\n",
        "\n",
        "def process_file(file_path, is_large=False):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if is_large:\n",
        "            # Process large file in chunks\n",
        "            chunks = pd.read_csv(file_path, chunksize=100000)  # Adjust chunksize as needed\n",
        "            processed_chunks = []\n",
        "            for chunk in tqdm(chunks, desc=f\"Processing {file_path}\"):\n",
        "                processed_chunk = process_chunk(chunk)\n",
        "                processed_chunks.append(processed_chunk)\n",
        "            df = pd.concat(processed_chunks, ignore_index=True)\n",
        "        else:\n",
        "            # Process small file\n",
        "            df = pd.read_csv(file_path)\n",
        "            df = process_chunk(df)\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Process both files\n",
        "file_paths = ['processed_file_1.csv', 'processed_file_2.csv']\n",
        "dataframes = []\n",
        "\n",
        "for i, file_path in enumerate(file_paths):\n",
        "    print(f\"Processing {file_path}...\")\n",
        "    df = process_file(file_path, is_large=(i == 1))  # Assume the second file is large\n",
        "    if df is not None:\n",
        "        dataframes.append(df)\n",
        "        print(f\"Shape of {file_path}: {df.shape}\")\n",
        "        print(f\"Columns in {file_path}: {df.columns.tolist()}\")\n",
        "        print(f\"Sample data from {file_path}:\\n{df.head()}\")\n",
        "        print(f\"Missing values in {file_path}:\\n{df.isnull().sum()}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Combine the dataframes if any were successfully processed\n",
        "if dataframes:\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "    print(f\"Shape of combined dataframe: {combined_df.shape}\")\n",
        "\n",
        "    # Save the processed and combined data\n",
        "    combined_df.to_csv('sampled_pubmed_data.csv', index=False)\n",
        "    print(\"Processed and combined data saved to 'sampled_pubmed_data.csv'\")\n",
        "else:\n",
        "    print(\"No dataframes were successfully processed.\")"
      ],
      "metadata": {
        "id": "fxV-fXYfyqGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}